---
layout: post
title: Cheat Sheet: Machine Learning Models
date: 2019-01-11 20:00:00
comments: true
author: Talita Shiguemoto
---




* [Supervised Learning](#supervised-learning)
	* [Naive Bayes](#naive-bayes)
	* [Decision Trees](#decision-trees)
	* [Support Vector Machines](#svm)
	* [Linear Regression](#linear-regression)
	* [K-Nearest Neighbors](#knn)
	* [Logistic Regression](#logistic-regression)
	* [Evaluating Models](#evaluating-sl)
* [Unsupervised Learning](#unsupervised-learning)
	* [KMeans](#kmeans)
	* [Hierarchical](#hierarchical)
	* [DBSAC](#dbsac)
	* [Gaussian Mixture Models](#gmm)
	* [Evaluating Models](#evaluating-ul)
* [Feature Scaling](#feature-scaling)
* [Principal Component Analysis](#pca)
* [Random Projection](#random-projection)
* [Independent Component Analysis](#ica)
* [](#)

## **Supervised Learning** {#supervised-learning}
<br/>
Supervised Learning are Machine Learning models that based on historical data, with outputs that we already know as correct, the algorithm is trained to be able to predict new results.

There are two types of problems for this type of learning:
    * **Regression**: we try to predict quantitative (numerical)
    * **Classification**: we try to predict qualitative (discrete)

### **Naive Bayes** {#naive-bayes}
<br/>


### **Decision Trees** {#decision-trees}
<br/>


**Term 1 - Supervised Learning**



**Term 2 - Unsupervised Learning**



**Term 3 - Reinforcement Learning**




### **Support Vector Machines** {#svm}
<br/>

### **Linear Regressions** {#linear-regression}
<br/>

### **K-Nearest Neighbors** {#knn}
<br/>

### **Logistic Regression** {#logistic-regression}
<br/>

### **Evaluating Models** {#evaluating-sl}
<br/>

## **Unupervised Learning** {#unsupervised-learning}
<br/>
Unsupervised Learning are Machine Learning models that have little or none historical data to be based on and to predict results, they do not need a dataset to tell which are the correct output variables to model a predictive algorithm. These models can create data structures based on relationships between variables or detect some trends with clustering.

### **KMeans** {#kmeans}
<br/>

### **Hierarchical** {#hierarchical}
<br/>

### **Density Based Spatial Clustering of Application with Noise** {#dbsac}
<br/>

### **Gaussian Mixture Models** {#gmm}
<br/>

### **Evaluating Models** {#evaluating-sl}
<br/>

### **Feature Scaling** {#feature-scaling}
<br/>

### **Principal Component Analysis** {#pca}
<br/>

### **Random Projection** {#random-projection}
<br/>

### **Independent Component Analysis** {#ica}
<br/>

## **Bibliography**
<br/>

Castle, Nikki. 2017. [Supervised vs. Unsupervised Machine Learning](https://www.datascience.com/blog/supervised-and-unsupervised-machine-learning-algorithms)

Çelik, Mete, et al. 2001.[Anomaly detection in temperature data using DBSCAN algorithm](https://ieeexplore.ieee.org/document/5946052)

Chen, Edwin. [Choosing a Machine Learning Classifier](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)

Erman, Jefreey, et al. 2006. [Traffic Classification Using Clustering Algorithms](https://conferences.sigcomm.org/sigcomm/2006/papers/minenet-01.pdf)

Gershman, Amir, et al. [A Decision Tree Based Recommender System](https://subs.emis.de/LNI/Proceedings/Proceedings165/170.pdf)

Grover, Prince. 2017. [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)

Huang, Min-Wei, et al. 2017. [SVM and SVM Ensembles in Breast Cancer
Prediction](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0161501&type=printable)

Klon, Anthony E., et al 2006. [Improved Naive Bayesian Modeling of Numerical Data for Absorption, Distribution, Metabolism and Excretion (ADME) Property Prediction](https://pubs.acs.org/doi/pdf/10.1021/ci0601315)

Markoski,Branko. 2015. [Application of AdaBoost Algorithm in
Basketball Player Detection](https://www.uni-obuda.hu/journal/Markoski_Ivankovic_Ratgeber_Pecev_Glusac_57.pdf)

Pattekari,Shadab Adam & Parveen, Asma. 2012. [PREDICTION SYSTEM FOR HEART DISEASE USING NAIVE BAYES](https://pdfs.semanticscholar.org/d32e/e90a5de89093a4fc95f43e0409cb91414726.pdf)

Portella, Letícia. 2018. [Machine Learning Models - My Cheat List](https://leportella.com/cheatlist/2018/05/20/models-cheat-list.html)

[Regressão Logística](https://edisciplinas.usp.br/pluginfile.php/3769787/mod_resource/content/1/09_RegressaoLogistica.pdf)

Sachanm Lalit, 2015. [Logistic Regression vs Decision Trees vs SVM: Part II](https://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/)

Saunders, Diane, et al. 2012. [Using hierarchical clustering of secreted protein families to classify and rank candidate effectors of rust fungi](https://www.ncbi.nlm.nih.gov/pubmed/22238666)

Simplilean. 2018. [Naive Bayes Classifier](https://www.slideshare.net/Simplilearn/naive-bayes-classifier-naive-bayes-algorithm-naive-bayes-classifier-with-example-simplilearn)

Simplilean. 2018. [KNN Algorithm](https://goo.gl/XP6xcp)

Singh, Aishwarya. 2018. [A Comprehensive Guide to Ensemble Learning (with Python codes)](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)

Sonagara, Darshan and Badheka, Soham. 2014. [Comparison of Basic Clustering Algorithms](https://pdfs.semanticscholar.org/f0a4/d6bfb37b6c1102f7ef6b0d0f2ef861da6aca.pdf)

Spencer, Melanie, et al. 2010. [Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency](https://www.ncbi.nlm.nih.gov/pubmed/21129376)

Statinfer, 2017. [SVM : Advantages Disadvantages and Applications](https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/)

Stauffer, Chris and Grmson W, E, L. [Adaptive background mixture models for real-time tracking](http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf)

Sun, Feng-Tso, et al. [Nonparametric Discovery of Human Routines from Sensor Data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.3152&rep=rep1&type=pdf)

[Sklearn documentation on Clustering Agglomerative](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)

[Sklearn documentation on Clustering K-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

[Sklearn documentation on Clustering DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Sklearn documentation on Decision Trees](http://scikit-learn.org/stable/modules/tree.html)

[Sklearn documentation on Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) 

[What is the difference between the the Gaussian, Bernoulli, Multinomial and the regular Naive Bayes algorithms?](https://www.quora.com/What-is-the-difference-between-the-the-Gaussian-Bernoulli-Multinomial-and-the-regular-Naive-Bayes-algorithms)